{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Contrastive Representation Distillation - CIFAR-100\n",
    "## Full Analysis: SupCon vs SupCRD vs Balanced vs Hybrid\n",
    "\n",
    "**Goal**: Comprehensive evaluation of contrastive distillation methods on CIFAR-100 with:\n",
    "- Î±/Î² hyperparameter sweeps\n",
    "- Temperature analysis\n",
    "- Balanced force normalization\n",
    "- Pull/push force dynamics\n",
    "- Semantic similarity validation\n",
    "- Hybrid loss optimization\n",
    "- **Switchable architectures**: ConvNet vs ResNet-18\n",
    "\n",
    "**Methods**:\n",
    "- **Baseline SupCon**: Standard supervised contrastive learning\n",
    "- **SupCRD**: Logit-weighted representation distillation (Î±, Î² tuning)\n",
    "- **Balanced SupCRD**: Force-normalized variant\n",
    "- **Hybrid**: Combined SupCon + SupCRD (Î» tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import display\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "os.makedirs(\"pth_models\", exist_ok=True)\n",
    "os.makedirs(\"json_results\", exist_ok=True)\n",
    "os.makedirs(\"json_results/training_logs\", exist_ok=True)\n",
    "print(\"âœ“ Created directories: plots/, pth_models/, json_results/\")\n",
    "\n",
    "# Device Config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "print(f\"Random seed set to 42 for reproducibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Architecture Selection & Hyperparameters\n",
    "\n",
    "**KEY CONFIGURATION**: Set your architecture choice here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ARCHITECTURE SELECTION (CHANGE HERE)\n",
    "# ============================================================\n",
    "TEACHER_ARCH = \"convnet\"  # Options: \"convnet\" or \"resnet18\"\n",
    "STUDENT_ARCH = \"convnet\"  # Options: \"convnet\" or \"resnet18\"\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "\n",
    "# Epoch settings (adjust based on architecture)\n",
    "if TEACHER_ARCH == \"resnet18\":\n",
    "    EPOCHS_TEACHER = 50  # ResNet needs more epochs\n",
    "    USE_LR_SCHEDULER = True\n",
    "else:\n",
    "    EPOCHS_TEACHER = 30  # ConvNet is faster\n",
    "    USE_LR_SCHEDULER = False\n",
    "\n",
    "EPOCHS_STUDENT = 20\n",
    "\n",
    "# ============================================================\n",
    "# CONTRASTIVE & DISTILLATION CONFIG\n",
    "# ============================================================\n",
    "TEMP = 0.07\n",
    "ALPHA = 1.0\n",
    "BETA = 10.0\n",
    "\n",
    "# Sweep ranges (reduced for faster iteration)\n",
    "ALPHA_SWEEP = [1.0, 2.0]  # Reduced from [1, 2, 3, 4, 5, 10, 50]\n",
    "BETA_SWEEP = [1.0, 12.0]  # Reduced from [1, 10, 12, 20, 50]\n",
    "TEMP_SWEEP = [0.05, 0.07]  # Reduced from [0.05, 0.07, 0.1, 0.15]\n",
    "LAMBDA_SWEEP = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# ============================================================\n",
    "# DATASET CONFIG (CIFAR-100)\n",
    "# ============================================================\n",
    "num_classes = 100\n",
    "\n",
    "# CIFAR-100 superclass mapping (20 superclasses, 5 classes each)\n",
    "cifar100_superclasses = [\n",
    "    'aquatic_mammals', 'fish', 'flowers', 'food_containers', 'fruit_vegetables',\n",
    "    'household_electrical', 'household_furniture', 'insects', 'large_carnivores',\n",
    "    'large_omnivores', 'medium_mammals', 'non-insect_invertebrates', 'people',\n",
    "    'reptiles', 'small_mammals', 'trees', 'vehicles_1', 'vehicles_2'\n",
    "]\n",
    "\n",
    "# Sample classes for visualization (not all 100)\n",
    "sample_classes = [0, 1, 2, 10, 11, 12, 20, 21, 22, 30]  # 10 representative classes\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CONFIGURATION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Teacher Architecture: {TEACHER_ARCH.upper()}\")\n",
    "print(f\"Student Architecture: {STUDENT_ARCH.upper()}\")\n",
    "print(f\"Dataset: CIFAR-100 ({num_classes} classes)\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LR}\")\n",
    "print(f\"Teacher Epochs: {EPOCHS_TEACHER}\")\n",
    "print(f\"Student Epochs: {EPOCHS_STUDENT}\")\n",
    "print(f\"LR Scheduler: {USE_LR_SCHEDULER}\")\n",
    "print(f\"Temperature: {TEMP}\")\n",
    "print(f\"Alpha Sweep: {ALPHA_SWEEP}\")\n",
    "print(f\"Beta Sweep: {BETA_SWEEP}\")\n",
    "print(f\"Temp Sweep: {TEMP_SWEEP}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Loading with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-100 mean and std\n",
    "cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
    "cifar100_std = (0.2675, 0.2565, 0.2761)\n",
    "\n",
    "# Training transform with augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar100_mean, cifar100_std)\n",
    "])\n",
    "\n",
    "# Test transform (no augmentation)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(cifar100_mean, cifar100_std)\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR100(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform_train\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR100(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform_test\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_set)}\")\n",
    "print(f\"Test samples: {len(test_set)}\")\n",
    "print(f\"Augmentation: RandomCrop, HFlip, ColorJitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Architecture (Switchable)\n",
    "\n",
    "Two architecture options:\n",
    "1. **ConvNet**: Fast, 3-layer CNN (same as CIFAR-10)\n",
    "2. **ResNet-18**: Deeper, more capacity, slower training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"Shallow 3-layer ConvNet encoder\"\"\"\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.flat_dim = 128 * 4 * 4\n",
    "        self.fc = nn.Linear(self.flat_dim, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"ResNet-18 encoder (adapted for CIFAR)\"\"\"\n",
    "    def __init__(self, feature_dim=512):\n",
    "        super().__init__()\n",
    "        # Use torchvision ResNet18 but modify first conv for CIFAR (32x32)\n",
    "        import torchvision.models as models\n",
    "        resnet = models.resnet18(pretrained=False)\n",
    "        \n",
    "        # Replace first conv: kernel 7->3, stride 2->1, remove maxpool\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        # Skip maxpool for CIFAR (small images)\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.avgpool = resnet.avgpool\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        # Project 512 -> feature_dim if different\n",
    "        if feature_dim != 512:\n",
    "            self.projection = nn.Linear(512, feature_dim)\n",
    "        else:\n",
    "            self.projection = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # No maxpool for CIFAR\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    \"\"\"Wrapper with switchable encoder\"\"\"\n",
    "    def __init__(self, num_classes=100, arch=\"convnet\"):\n",
    "        super().__init__()\n",
    "        self.arch = arch\n",
    "        \n",
    "        if arch == \"convnet\":\n",
    "            self.encoder = ConvEncoder(feature_dim=128)\n",
    "            self.feature_dim = 128\n",
    "        elif arch == \"resnet18\":\n",
    "            self.encoder = ResNetEncoder(feature_dim=512)\n",
    "            self.feature_dim = 512\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown architecture: {arch}\")\n",
    "        \n",
    "        # Projector (for contrastive learning)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(self.feature_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)\n",
    "        proj = self.projector(feats)\n",
    "        logits = self.classifier(feats)\n",
    "        return feats, proj, logits\n",
    "\n",
    "\n",
    "# Test instantiation\n",
    "test_teacher = ModelWrapper(num_classes=100, arch=TEACHER_ARCH).to(device)\n",
    "test_student = ModelWrapper(num_classes=100, arch=STUDENT_ARCH).to(device)\n",
    "\n",
    "print(f\"âœ“ Model architectures defined\")\n",
    "print(f\"  Teacher: {TEACHER_ARCH.upper()} ({test_teacher.feature_dim}-dim features)\")\n",
    "print(f\"  Student: {STUDENT_ARCH.upper()} ({test_student.feature_dim}-dim features)\")\n",
    "\n",
    "# Count parameters\n",
    "teacher_params = sum(p.numel() for p in test_teacher.parameters())\n",
    "student_params = sum(p.numel() for p in test_student.parameters())\n",
    "print(f\"  Teacher params: {teacher_params:,}\")\n",
    "print(f\"  Student params: {student_params:,}\")\n",
    "\n",
    "del test_teacher, test_student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Baseline Supervised Contrastive Loss\"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temp = temperature\n",
    "\n",
    "    def forward(self, student_proj, labels):\n",
    "        feats = F.normalize(student_proj, dim=1)\n",
    "        sim_matrix = torch.matmul(feats, feats.T) / self.temp\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask), 1,\n",
    "            torch.arange(feats.shape[0]).view(-1, 1).to(device), 0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "        logits_max, _ = torch.max(sim_matrix, dim=1, keepdim=True)\n",
    "        sim_matrix = sim_matrix - logits_max.detach()\n",
    "        exp_logits = torch.exp(sim_matrix) * logits_mask\n",
    "        log_prob = sim_matrix - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n",
    "        return -mean_log_prob_pos.mean()\n",
    "\n",
    "\n",
    "class LogitWeightedSupCRDLoss(nn.Module):\n",
    "    \"\"\"Logit-Weighted SupCRD with Î±/Î² weighting\"\"\"\n",
    "    def __init__(self, alpha=1.0, beta=1.0, temperature=0.07, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.tau = temperature\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, student_features, teacher_features, teacher_logits, labels):\n",
    "        batch_size = student_features.shape[0]\n",
    "        device = student_features.device\n",
    "        s_norm = F.normalize(student_features, dim=1)\n",
    "        t_norm = F.normalize(teacher_features, dim=1)\n",
    "        sim_matrix = torch.matmul(s_norm, t_norm.T) / self.tau\n",
    "        sim_max, _ = torch.max(sim_matrix, dim=1, keepdim=True)\n",
    "        sim_matrix = sim_matrix - sim_max.detach()\n",
    "        exp_sim = torch.exp(sim_matrix)\n",
    "        teacher_probs = F.softmax(teacher_logits, dim=1)\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask_pos = torch.eq(labels, labels.T).float().to(device)\n",
    "        mask_neg = 1.0 - mask_pos\n",
    "        p_target = torch.gather(teacher_probs, 1, labels).view(-1)\n",
    "        w_pull = self.alpha * p_target\n",
    "        target_labels_expand = labels.view(1, -1).expand(batch_size, -1)\n",
    "        p_negative_class = torch.gather(teacher_probs, 1, target_labels_expand)\n",
    "        w_push = self.beta * (1.0 - p_negative_class)\n",
    "        sum_pos_exp = (exp_sim * mask_pos).sum(dim=1)\n",
    "        numerator_term = w_pull * sum_pos_exp\n",
    "        weighted_neg_exp = (exp_sim * w_push * mask_neg).sum(dim=1)\n",
    "        denominator_term = numerator_term + weighted_neg_exp\n",
    "        loss = -torch.log((numerator_term + self.eps) / (denominator_term + self.eps))\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class BalancedLogitWeightedSupCRDLoss(nn.Module):\n",
    "    \"\"\"Balanced SupCRD with automatic force normalization\"\"\"\n",
    "    def __init__(self, alpha=1.0, beta=1.0, temperature=0.07, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.tau = temperature\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, student_features, teacher_features, teacher_logits, labels):\n",
    "        batch_size = student_features.shape[0]\n",
    "        device = student_features.device\n",
    "        s_norm = F.normalize(student_features, dim=1)\n",
    "        t_norm = F.normalize(teacher_features, dim=1)\n",
    "        sim_matrix = torch.matmul(s_norm, t_norm.T) / self.tau\n",
    "        sim_max, _ = torch.max(sim_matrix, dim=1, keepdim=True)\n",
    "        sim_matrix = sim_matrix - sim_max.detach()\n",
    "        exp_sim = torch.exp(sim_matrix)\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask_pos = torch.eq(labels, labels.T).float().to(device)\n",
    "        mask_neg = 1.0 - mask_pos\n",
    "        teacher_probs = F.softmax(teacher_logits, dim=1)\n",
    "        p_target = torch.gather(teacher_probs, 1, labels).view(-1)\n",
    "        w_pull = self.alpha * p_target\n",
    "        target_labels_expand = labels.view(1, -1).expand(batch_size, -1)\n",
    "        p_negative_class = torch.gather(teacher_probs, 1, target_labels_expand)\n",
    "        w_push_raw = self.beta * (1.0 - p_negative_class)\n",
    "        total_push_mass = (w_push_raw * mask_neg).sum(dim=1)\n",
    "        norm_factor = w_pull / (total_push_mass + self.eps)\n",
    "        w_push_balanced = w_push_raw * norm_factor.view(-1, 1)\n",
    "        sum_pos_exp = (exp_sim * mask_pos).sum(dim=1)\n",
    "        numerator_term = w_pull * sum_pos_exp\n",
    "        weighted_neg_exp = (exp_sim * w_push_balanced * mask_neg).sum(dim=1)\n",
    "        denominator_term = numerator_term + weighted_neg_exp\n",
    "        loss = -torch.log((numerator_term + self.eps) / (denominator_term + self.eps))\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class HybridSupCRDLoss(nn.Module):\n",
    "    \"\"\"Hybrid: Î» * SupCon + (1-Î») * SupCRD\"\"\"\n",
    "    def __init__(self, alpha=1.0, beta=12.0, lambda_supcon=0.7, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.supcon_loss = SupConLoss(temperature=temperature)\n",
    "        self.supcrd_loss = LogitWeightedSupCRDLoss(alpha=alpha, beta=beta, temperature=temperature)\n",
    "        self.lambda_supcon = lambda_supcon\n",
    "        self.register_buffer(\"supcon_scale\", torch.tensor(1.0))\n",
    "        self.register_buffer(\"supcrd_scale\", torch.tensor(1.0))\n",
    "        self.warmup_steps = 100\n",
    "        self.step_count = 0\n",
    "\n",
    "    def forward(self, student_proj, teacher_proj, teacher_logits, labels):\n",
    "        loss_supcon = self.supcon_loss(student_proj, labels)\n",
    "        loss_supcrd = self.supcrd_loss(student_proj, teacher_proj, teacher_logits, labels)\n",
    "        if self.step_count < self.warmup_steps:\n",
    "            self.step_count += 1\n",
    "            with torch.no_grad():\n",
    "                self.supcon_scale = 0.9 * self.supcon_scale + 0.1 * loss_supcon.detach()\n",
    "                self.supcrd_scale = 0.9 * self.supcrd_scale + 0.1 * loss_supcrd.detach()\n",
    "        loss_supcon_norm = loss_supcon / (self.supcon_scale + 1e-8)\n",
    "        loss_supcrd_norm = loss_supcrd / (self.supcrd_scale + 1e-8)\n",
    "        return (self.lambda_supcon * loss_supcon_norm + (1 - self.lambda_supcon) * loss_supcrd_norm)\n",
    "\n",
    "\n",
    "print(\"âœ“ Loss functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Evaluate model accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            _, _, logits = model(images)\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    return 100. * correct / total\n",
    "\n",
    "\n",
    "def extract_features_and_labels(model, loader, device, max_samples=5000):\n",
    "    \"\"\"Extract features and labels for visualization.\"\"\"\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            if count >= max_samples:\n",
    "                break\n",
    "            images = images.to(device)\n",
    "            feats, proj, _ = model(images)\n",
    "            features_list.append(proj.cpu().numpy())\n",
    "            labels_list.append(labels.numpy())\n",
    "            count += images.size(0)\n",
    "    \n",
    "    features = np.concatenate(features_list, axis=0)[:max_samples]\n",
    "    labels = np.concatenate(labels_list, axis=0)[:max_samples]\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def visualize_latents(model, loader, device, title=\"\", sample_classes=None, max_samples=5000):\n",
    "    \"\"\"Visualize latent space with t-SNE (sample subset for CIFAR-100).\"\"\"\n",
    "    features, labels = extract_features_and_labels(model, loader, device, max_samples)\n",
    "    \n",
    "    # If sample_classes specified, only visualize those\n",
    "    if sample_classes is not None:\n",
    "        mask = np.isin(labels, sample_classes)\n",
    "        features = features[mask]\n",
    "        labels = labels[mask]\n",
    "        print(f\"  Visualizing {len(sample_classes)} classes, {len(features)} samples\")\n",
    "    \n",
    "    print(f\"  Running t-SNE on {len(features)} samples...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features)-1))\n",
    "    embedded = tsne.fit_transform(features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        mask = labels == label\n",
    "        plt.scatter(embedded[mask, 0], embedded[mask, 1], \n",
    "                   label=f'Class {label}', alpha=0.6, s=20)\n",
    "    \n",
    "    plt.title(f't-SNE: {title}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    if len(unique_labels) <= 20:\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    safe_title = title.replace(' ', '_').replace('/', '_')\n",
    "    plt.savefig(f'plots/tsne_{safe_title}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_class_centroids(model, loader, device, num_classes=100):\n",
    "    \"\"\"Compute class centroids in feature space.\"\"\"\n",
    "    model.eval()\n",
    "    centroids = {i: [] for i in range(num_classes)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            feats, proj, _ = model(images)\n",
    "            proj_norm = F.normalize(proj, dim=1)\n",
    "            \n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                centroids[label].append(proj_norm[i].cpu().numpy())\n",
    "    \n",
    "    # Average to get centroid\n",
    "    for cls in centroids:\n",
    "        if len(centroids[cls]) > 0:\n",
    "            centroids[cls] = np.mean(centroids[cls], axis=0)\n",
    "        else:\n",
    "            centroids[cls] = np.zeros(64)  # Projection dim\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "\n",
    "def analyze_similarity(model, loader, device, class_pairs, title=\"\"):\n",
    "    \"\"\"Analyze cosine similarity between specific class pairs.\"\"\"\n",
    "    centroids = compute_class_centroids(model, loader, device)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Semantic Similarity Analysis: {title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {}\n",
    "    for cls1, cls2, desc in class_pairs:\n",
    "        c1 = centroids[cls1]\n",
    "        c2 = centroids[cls2]\n",
    "        similarity = np.dot(c1, c2) / (np.linalg.norm(c1) * np.linalg.norm(c2) + 1e-8)\n",
    "        results[f\"{cls1}-{cls2}\"] = similarity\n",
    "        print(f\"  {desc:30s}: {similarity:.3f}\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_training_log(log_data, filename):\n",
    "    \"\"\"Save training log to JSON.\"\"\"\n",
    "    with open(f'json_results/training_logs/{filename}.json', 'w') as f:\n",
    "        json.dump(log_data, f, indent=2)\n",
    "\n",
    "\n",
    "def load_training_log(filename):\n",
    "    \"\"\"Load training log from JSON.\"\"\"\n",
    "    path = f'json_results/training_logs/{filename}.json'\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"âœ“ Utility functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher(teacher, train_loader, optimizer, criterion, device, \n",
    "                 epochs=10, scheduler=None, log_name=None):\n",
    "    \"\"\"Train teacher with optional LR scheduling.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING TEACHER MODEL ({epochs} epochs)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    training_log = {'epochs': [], 'train_loss': [], 'train_acc': []}\n",
    "    teacher.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            _, _, logits = teacher(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        acc = 100. * correct / total\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs}: Loss={avg_loss:.3f} | Acc={acc:.1f}% | LR={lr:.6f}\")\n",
    "        \n",
    "        training_log['epochs'].append(epoch + 1)\n",
    "        training_log['train_loss'].append(avg_loss)\n",
    "        training_log['train_acc'].append(acc)\n",
    "    \n",
    "    if log_name:\n",
    "        save_training_log(training_log, log_name)\n",
    "    \n",
    "    print(f\"\\nâœ“ Teacher training complete: {acc:.1f}% accuracy\\n\")\n",
    "    return teacher, training_log\n",
    "\n",
    "\n",
    "def train_student(teacher, student, train_loader, optimizer, criterion, \n",
    "                 device, epochs=20, label=\"\", mode=\"supcrd\", log_name=None):\n",
    "    \"\"\"Train student with logging.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING: {label} (mode={mode})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    \n",
    "    # Linear classifier on frozen features\n",
    "    linear_classifier = nn.Linear(student.feature_dim, 100).to(device)\n",
    "    classifier_opt = torch.optim.Adam(linear_classifier.parameters(), lr=LR)\n",
    "    classifier_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    training_log = {'epochs': [], 'contrastive_loss': [], 'train_acc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels_batch in train_loader:\n",
    "            images, labels_batch = images.to(device), labels_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_features = teacher.encoder(images)\n",
    "                teacher_proj = teacher.projector(teacher_features)\n",
    "                teacher_logits = teacher.classifier(teacher_features)\n",
    "            \n",
    "            student_features = student.encoder(images)\n",
    "            student_proj = student.projector(student_features)\n",
    "            \n",
    "            if mode == 'supcon':\n",
    "                loss = criterion(student_proj, labels_batch)\n",
    "            elif mode in ['supcrd', 'hybrid', 'balanced']:\n",
    "                loss = criterion(student_proj, teacher_proj, teacher_logits, labels_batch)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown mode: {mode}\")\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Train linear classifier on frozen features\n",
    "            with torch.no_grad():\n",
    "                frozen_features = student.encoder(images)\n",
    "            logits = linear_classifier(frozen_features)\n",
    "            clf_loss = classifier_criterion(logits, labels_batch)\n",
    "            classifier_opt.zero_grad()\n",
    "            clf_loss.backward()\n",
    "            classifier_opt.step()\n",
    "            \n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels_batch.size(0)\n",
    "            correct += predicted.eq(labels_batch).sum().item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        acc = 100. * correct / total\n",
    "        print(f\"  [{label}] Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Acc: {acc:.1f}%\")\n",
    "        \n",
    "        training_log['epochs'].append(epoch + 1)\n",
    "        training_log['contrastive_loss'].append(avg_loss)\n",
    "        training_log['train_acc'].append(acc)\n",
    "    \n",
    "    # Copy trained classifier to student\n",
    "    student.classifier.load_state_dict(linear_classifier.state_dict())\n",
    "    \n",
    "    if log_name:\n",
    "        save_training_log(training_log, log_name)\n",
    "    \n",
    "    print(f\"\\nâœ“ {label} training complete\\n\")\n",
    "    return student, training_log\n",
    "\n",
    "\n",
    "print(\"âœ“ Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXPERIMENT 1: Train Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RETRAIN_TEACHER = False\n",
    "teacher_model_path = f\"pth_models/teacher_{TEACHER_ARCH}_cifar100.pth\"\n",
    "\n",
    "teacher_log = load_training_log(f\"teacher_{TEACHER_ARCH}_cifar100\")\n",
    "if os.path.exists(teacher_model_path) and not FORCE_RETRAIN_TEACHER and teacher_log:\n",
    "    print(f\"Loading teacher from {teacher_model_path}\")\n",
    "    teacher = ModelWrapper(num_classes=num_classes, arch=TEACHER_ARCH).to(device)\n",
    "    teacher.load_state_dict(torch.load(teacher_model_path, map_location=device))\n",
    "    print(f\"  Train Acc (final): {teacher_log['train_acc'][-1]:.2f}%\")\n",
    "else:\n",
    "    teacher = ModelWrapper(num_classes=num_classes, arch=TEACHER_ARCH).to(device)\n",
    "    optimizer_teacher = torch.optim.Adam(teacher.parameters(), lr=LR)\n",
    "    criterion_teacher = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Optional: LR scheduler for longer training\n",
    "    scheduler_teacher = None\n",
    "    if USE_LR_SCHEDULER:\n",
    "        scheduler_teacher = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer_teacher, T_max=EPOCHS_TEACHER\n",
    "        )\n",
    "        print(\"Using CosineAnnealingLR scheduler\")\n",
    "    \n",
    "    teacher, teacher_log = train_teacher(\n",
    "        teacher,\n",
    "        train_loader,\n",
    "        optimizer_teacher,\n",
    "        criterion_teacher,\n",
    "        device,\n",
    "        epochs=EPOCHS_TEACHER,\n",
    "        scheduler=scheduler_teacher,\n",
    "        log_name=f\"teacher_{TEACHER_ARCH}_cifar100\",\n",
    "    )\n",
    "    torch.save(teacher.state_dict(), teacher_model_path)\n",
    "\n",
    "acc_teacher = evaluate_model(teacher, test_loader, device)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Teacher Test Accuracy: {acc_teacher:.2f}%\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "# Visualize teacher's latent space (sample classes only)\n",
    "print(\"ðŸ“Š Visualizing teacher's latent space...\")\n",
    "visualize_latents(\n",
    "    teacher, \n",
    "    test_loader, \n",
    "    device, \n",
    "    title=f\"Teacher_{TEACHER_ARCH}_CIFAR100\",\n",
    "    sample_classes=sample_classes\n",
    ")\n",
    "print(\"âœ“ Teacher visualization complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXPERIMENT 2: Baseline SupCon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RETRAIN_BASELINE = False\n",
    "baseline_model_path = f'pth_models/student_baseline_supcon_{STUDENT_ARCH}_cifar100.pth'\n",
    "\n",
    "baseline_log = load_training_log(f'baseline_supcon_{STUDENT_ARCH}_cifar100')\n",
    "if os.path.exists(baseline_model_path) and not FORCE_RETRAIN_BASELINE and baseline_log:\n",
    "    print(f\"Loading baseline from {baseline_model_path}\")\n",
    "    student_baseline = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "    student_baseline.load_state_dict(torch.load(baseline_model_path, map_location=device))\n",
    "    print(f\"  Train Acc (final): {baseline_log['train_acc'][-1]:.2f}%\")\n",
    "else:\n",
    "    student_baseline = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "    optimizer_baseline = torch.optim.Adam(student_baseline.parameters(), lr=LR)\n",
    "    criterion_baseline = SupConLoss(temperature=TEMP)\n",
    "    student_baseline, baseline_log = train_student(\n",
    "        teacher, student_baseline, train_loader, optimizer_baseline,\n",
    "        criterion_baseline, device, epochs=EPOCHS_STUDENT,\n",
    "        label=\"Baseline_SupCon\", mode='supcon', \n",
    "        log_name=f'baseline_supcon_{STUDENT_ARCH}_cifar100'\n",
    "    )\n",
    "    torch.save(student_baseline.state_dict(), baseline_model_path)\n",
    "\n",
    "acc_baseline = evaluate_model(student_baseline, test_loader, device)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Baseline SupCon Test Accuracy: {acc_baseline:.2f}%\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "\n",
    "# Visualize baseline's latent space\n",
    "print(\"ðŸ“Š Visualizing baseline's latent space...\")\n",
    "visualize_latents(\n",
    "    student_baseline, \n",
    "    test_loader, \n",
    "    device, \n",
    "    title=f\"Baseline_SupCon_{STUDENT_ARCH}_CIFAR100\",\n",
    "    sample_classes=sample_classes\n",
    ")\n",
    "print(\"âœ“ Baseline visualization complete\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXPERIMENT 3: Alpha Sweep (Î²=1 fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RETRAIN_ALPHA_SWEEP = False\n",
    "\n",
    "alpha_results = {}\n",
    "for alpha_val in ALPHA_SWEEP:\n",
    "    exp_name = f'alpha_{alpha_val}_beta_1.0'\n",
    "    model_path = f'pth_models/student_{exp_name}_{STUDENT_ARCH}_cifar100.pth'\n",
    "    \n",
    "    log = load_training_log(f'{exp_name}_{STUDENT_ARCH}_cifar100')\n",
    "    if os.path.exists(model_path) and not FORCE_RETRAIN_ALPHA_SWEEP and log:\n",
    "        print(f\"Loading {exp_name} from cache\")\n",
    "        student = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "        student.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        train_acc = log['train_acc'][-1]\n",
    "    else:\n",
    "        student = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "        optimizer = torch.optim.Adam(student.parameters(), lr=LR)\n",
    "        criterion = LogitWeightedSupCRDLoss(alpha=alpha_val, beta=1.0, temperature=TEMP)\n",
    "        student, log = train_student(\n",
    "            teacher, student, train_loader, optimizer, criterion, device,\n",
    "            epochs=EPOCHS_STUDENT, label=f'SupCRD_Î±={alpha_val}', mode='supcrd',\n",
    "            log_name=f'{exp_name}_{STUDENT_ARCH}_cifar100'\n",
    "        )\n",
    "        torch.save(student.state_dict(), model_path)\n",
    "        train_acc = log['train_acc'][-1]\n",
    "    \n",
    "    test_acc = evaluate_model(student, test_loader, device)\n",
    "    alpha_results[alpha_val] = {'train': train_acc, 'test': test_acc}\n",
    "    print(f\"Î±={alpha_val}: Train={train_acc:.1f}%, Test={test_acc:.1f}%\\n\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALPHA SWEEP SUMMARY (Î²=1.0, Ï„={TEMP})\")\n",
    "print(f\"{'='*60}\")\n",
    "for alpha_val, res in alpha_results.items():\n",
    "    delta = res['test'] - acc_baseline\n",
    "    print(f\"Î±={alpha_val:5.1f}: Test={res['test']:5.2f}% (Î”={delta:+.2f}%)\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXPERIMENT 4: Beta Sweep (Î±=1 fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RETRAIN_BETA_SWEEP = False\n",
    "\n",
    "beta_results = {}\n",
    "for beta_val in BETA_SWEEP:\n",
    "    exp_name = f'alpha_1.0_beta_{beta_val}'\n",
    "    model_path = f'pth_models/student_{exp_name}_{STUDENT_ARCH}_cifar100.pth'\n",
    "    \n",
    "    log = load_training_log(f'{exp_name}_{STUDENT_ARCH}_cifar100')\n",
    "    if os.path.exists(model_path) and not FORCE_RETRAIN_BETA_SWEEP and log:\n",
    "        print(f\"Loading {exp_name} from cache\")\n",
    "        student = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "        student.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        train_acc = log['train_acc'][-1]\n",
    "    else:\n",
    "        student = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "        optimizer = torch.optim.Adam(student.parameters(), lr=LR)\n",
    "        criterion = LogitWeightedSupCRDLoss(alpha=1.0, beta=beta_val, temperature=TEMP)\n",
    "        student, log = train_student(\n",
    "            teacher, student, train_loader, optimizer, criterion, device,\n",
    "            epochs=EPOCHS_STUDENT, label=f'SupCRD_Î²={beta_val}', mode='supcrd',\n",
    "            log_name=f'{exp_name}_{STUDENT_ARCH}_cifar100'\n",
    "        )\n",
    "        torch.save(student.state_dict(), model_path)\n",
    "        train_acc = log['train_acc'][-1]\n",
    "    \n",
    "    test_acc = evaluate_model(student, test_loader, device)\n",
    "    beta_results[beta_val] = {'train': train_acc, 'test': test_acc}\n",
    "    print(f\"Î²={beta_val}: Train={train_acc:.1f}%, Test={test_acc:.1f}%\\n\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BETA SWEEP SUMMARY (Î±=1.0, Ï„={TEMP})\")\n",
    "print(f\"{'='*60}\")\n",
    "for beta_val, res in beta_results.items():\n",
    "    delta = res['test'] - acc_baseline\n",
    "    print(f\"Î²={beta_val:5.1f}: Test={res['test']:5.2f}% (Î”={delta:+.2f}%)\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXPERIMENT 5: Temperature Sweep (Î±=1, Î²=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RETRAIN_TEMP_SWEEP = False\n",
    "\n",
    "temp_results = {}\n",
    "for temp_val in TEMP_SWEEP:\n",
    "    exp_name = f'alpha_1.0_beta_12.0_temp_{temp_val}'\n",
    "    model_path = f'pth_models/student_{exp_name}_{STUDENT_ARCH}_cifar100.pth'\n",
    "    \n",
    "    log = load_training_log(f'{exp_name}_{STUDENT_ARCH}_cifar100')\n",
    "    if os.path.exists(model_path) and not FORCE_RETRAIN_TEMP_SWEEP and log:\n",
    "        print(f\"Loading {exp_name} from cache\")\n",
    "        student = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "        student.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        train_acc = log['train_acc'][-1]\n",
    "    else:\n",
    "        student = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "        optimizer = torch.optim.Adam(student.parameters(), lr=LR)\n",
    "        criterion = LogitWeightedSupCRDLoss(alpha=1.0, beta=12.0, temperature=temp_val)\n",
    "        student, log = train_student(\n",
    "            teacher, student, train_loader, optimizer, criterion, device,\n",
    "            epochs=EPOCHS_STUDENT, label=f'SupCRD_Ï„={temp_val}', mode='supcrd',\n",
    "            log_name=f'{exp_name}_{STUDENT_ARCH}_cifar100'\n",
    "        )\n",
    "        torch.save(student.state_dict(), model_path)\n",
    "        train_acc = log['train_acc'][-1]\n",
    "    \n",
    "    test_acc = evaluate_model(student, test_loader, device)\n",
    "    temp_results[temp_val] = {'train': train_acc, 'test': test_acc}\n",
    "    print(f\"Ï„={temp_val}: Train={train_acc:.1f}%, Test={test_acc:.1f}%\\n\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TEMPERATURE SWEEP SUMMARY (Î±=1.0, Î²=12.0)\")\n",
    "print(f\"{'='*60}\")\n",
    "for temp_val, res in temp_results.items():\n",
    "    delta = res['test'] - acc_baseline\n",
    "    print(f\"Ï„={temp_val:5.2f}: Test={res['test']:5.2f}% (Î”={delta:+.2f}%)\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXPERIMENT 6: Balanced SupCRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RETRAIN_BALANCED = False\n",
    "balanced_model_path = f'pth_models/student_balanced_{STUDENT_ARCH}_cifar100.pth'\n",
    "\n",
    "balanced_log = load_training_log(f'balanced_{STUDENT_ARCH}_cifar100')\n",
    "if os.path.exists(balanced_model_path) and not FORCE_RETRAIN_BALANCED and balanced_log:\n",
    "    print(f\"Loading balanced from {balanced_model_path}\")\n",
    "    student_balanced = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "    student_balanced.load_state_dict(torch.load(balanced_model_path, map_location=device))\n",
    "    train_acc = balanced_log['train_acc'][-1]\n",
    "else:\n",
    "    student_balanced = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "    optimizer_balanced = torch.optim.Adam(student_balanced.parameters(), lr=LR)\n",
    "    criterion_balanced = BalancedLogitWeightedSupCRDLoss(alpha=1.0, beta=12.0, temperature=TEMP)\n",
    "    student_balanced, balanced_log = train_student(\n",
    "        teacher, student_balanced, train_loader, optimizer_balanced,\n",
    "        criterion_balanced, device, epochs=EPOCHS_STUDENT,\n",
    "        label=\"Balanced_SupCRD\", mode='balanced',\n",
    "        log_name=f'balanced_{STUDENT_ARCH}_cifar100'\n",
    "    )\n",
    "    torch.save(student_balanced.state_dict(), balanced_model_path)\n",
    "    train_acc = balanced_log['train_acc'][-1]\n",
    "\n",
    "acc_balanced = evaluate_model(student_balanced, test_loader, device)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Balanced SupCRD Test Accuracy: {acc_balanced:.2f}%\")\n",
    "print(f\"Train Accuracy: {train_acc:.2f}%\")\n",
    "print(f\"{'='*50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXPERIMENT 7: Semantic Similarity Analysis\n",
    "\n",
    "Compare semantic structure for selected class pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define representative class pairs for CIFAR-100\n",
    "# Format: (class1_idx, class2_idx, description)\n",
    "class_pairs = [\n",
    "    (3, 42, \"baby-boy (similar)\"),\n",
    "    (3, 99, \"baby-worm (dissimilar)\"),\n",
    "    (4, 55, \"bear-otter (similar mammals)\"),\n",
    "    (4, 8, \"bear-bicycle (dissimilar)\"),\n",
    "    (11, 35, \"boy-girl (similar)\"),\n",
    "    (11, 90, \"boy-tulip (dissimilar)\"),\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SEMANTIC SIMILARITY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze baseline\n",
    "baseline_sim = analyze_similarity(\n",
    "    student_baseline, test_loader, device, \n",
    "    class_pairs, title=\"Baseline SupCon\"\n",
    ")\n",
    "\n",
    "# Analyze best SupCRD (choose best from sweeps)\n",
    "best_alpha = max(alpha_results.items(), key=lambda x: x[1]['test'])[0]\n",
    "best_supcrd_path = f'pth_models/student_alpha_{best_alpha}_beta_1.0_{STUDENT_ARCH}_cifar100.pth'\n",
    "if os.path.exists(best_supcrd_path):\n",
    "    student_best = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "    student_best.load_state_dict(torch.load(best_supcrd_path, map_location=device))\n",
    "    best_sim = analyze_similarity(\n",
    "        student_best, test_loader, device,\n",
    "        class_pairs, title=f\"SupCRD (Î±={best_alpha}, Î²=1)\"\n",
    "    )\n",
    "\n",
    "# Analyze balanced\n",
    "balanced_sim = analyze_similarity(\n",
    "    student_balanced, test_loader, device,\n",
    "    class_pairs, title=\"Balanced SupCRD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EXPERIMENT 8: Hybrid Loss (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORCE_RETRAIN_HYBRID = False\n",
    "RUN_HYBRID_EXPERIMENTS = False  # Set to True to run hybrid experiments\n",
    "\n",
    "if RUN_HYBRID_EXPERIMENTS:\n",
    "    hybrid_results = {}\n",
    "    for lambda_val in LAMBDA_SWEEP:\n",
    "        exp_name = f'hybrid_lambda_{lambda_val}'\n",
    "        model_path = f'pth_models/student_{exp_name}_{STUDENT_ARCH}_cifar100.pth'\n",
    "        \n",
    "        log = load_training_log(f'{exp_name}_{STUDENT_ARCH}_cifar100')\n",
    "        if os.path.exists(model_path) and not FORCE_RETRAIN_HYBRID and log:\n",
    "            print(f\"Loading {exp_name} from cache\")\n",
    "            student = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "            student.load_state_dict(torch.load(model_path, map_location=device))\n",
    "            train_acc = log['train_acc'][-1]\n",
    "        else:\n",
    "            student = ModelWrapper(num_classes=num_classes, arch=STUDENT_ARCH).to(device)\n",
    "            optimizer = torch.optim.Adam(student.parameters(), lr=LR)\n",
    "            criterion = HybridSupCRDLoss(alpha=1.0, beta=12.0, lambda_supcon=lambda_val, temperature=TEMP)\n",
    "            student, log = train_student(\n",
    "                teacher, student, train_loader, optimizer, criterion, device,\n",
    "                epochs=EPOCHS_STUDENT, label=f'Hybrid_Î»={lambda_val}', mode='hybrid',\n",
    "                log_name=f'{exp_name}_{STUDENT_ARCH}_cifar100'\n",
    "            )\n",
    "            torch.save(student.state_dict(), model_path)\n",
    "            train_acc = log['train_acc'][-1]\n",
    "        \n",
    "        test_acc = evaluate_model(student, test_loader, device)\n",
    "        hybrid_results[lambda_val] = {'train': train_acc, 'test': test_acc}\n",
    "        print(f\"Î»={lambda_val}: Train={train_acc:.1f}%, Test={test_acc:.1f}%\\n\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"HYBRID LOSS SUMMARY (Î±=1.0, Î²=12.0, Ï„={TEMP})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for lambda_val, res in hybrid_results.items():\n",
    "        delta = res['test'] - acc_baseline\n",
    "        print(f\"Î»={lambda_val:5.2f}: Test={res['test']:5.2f}% (Î”={delta:+.2f}%)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(\"Skipping hybrid experiments (RUN_HYBRID_EXPERIMENTS=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Summary & Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile comprehensive results\n",
    "comprehensive_results = {\n",
    "    'config': {\n",
    "        'dataset': 'CIFAR-100',\n",
    "        'teacher_arch': TEACHER_ARCH,\n",
    "        'student_arch': STUDENT_ARCH,\n",
    "        'epochs_teacher': EPOCHS_TEACHER,\n",
    "        'epochs_student': EPOCHS_STUDENT,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'lr': LR,\n",
    "        'temperature': TEMP,\n",
    "    },\n",
    "    'teacher': {\n",
    "        'test_acc': acc_teacher,\n",
    "    },\n",
    "    'baseline_supcon': {\n",
    "        'test_acc': acc_baseline,\n",
    "    },\n",
    "    'alpha_sweep': alpha_results,\n",
    "    'beta_sweep': beta_results,\n",
    "    'temp_sweep': temp_results,\n",
    "    'balanced': {\n",
    "        'test_acc': acc_balanced,\n",
    "    },\n",
    "}\n",
    "\n",
    "if RUN_HYBRID_EXPERIMENTS:\n",
    "    comprehensive_results['hybrid_sweep'] = hybrid_results\n",
    "\n",
    "# Save to JSON\n",
    "results_path = f'json_results/comprehensive_results_{STUDENT_ARCH}_cifar100.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(comprehensive_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"FINAL SUMMARY - CIFAR-100\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Architecture: Teacher={TEACHER_ARCH.upper()}, Student={STUDENT_ARCH.upper()}\")\n",
    "print(f\"Teacher:        {acc_teacher:.2f}%\")\n",
    "print(f\"Baseline:       {acc_baseline:.2f}%\")\n",
    "print(f\"Balanced:       {acc_balanced:.2f}% (Î”={acc_balanced-acc_baseline:+.2f}%)\")\n",
    "print(f\"\\nBest Î± config:  {max(alpha_results.items(), key=lambda x: x[1]['test'])[0]} â†’ {max(alpha_results.values(), key=lambda x: x['test'])['test']:.2f}%\")\n",
    "print(f\"Best Î² config:  {max(beta_results.items(), key=lambda x: x[1]['test'])[0]} â†’ {max(beta_results.values(), key=lambda x: x['test'])['test']:.2f}%\")\n",
    "print(f\"Best Ï„ config:  {max(temp_results.items(), key=lambda x: x[1]['test'])[0]} â†’ {max(temp_results.values(), key=lambda x: x['test'])['test']:.2f}%\")\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
