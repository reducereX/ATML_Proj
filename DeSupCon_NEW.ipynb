{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Representation Distillation Experiments\n",
    "## Comparing Baseline SupCon vs Logit-Weighted SupCRD\n",
    "\n",
    "**Goal**: Evaluate whether using teacher logits to weight contrastive forces improves student representation quality.\n",
    "\n",
    "**Methods**:\n",
    "- **Baseline**: Standard Supervised Contrastive Learning (SupCon)\n",
    "- **Proposed**: Logit-Weighted Supervised Contrastive Representation Distillation (SupCRD)\n",
    "  - Pull weight: `α × P_teacher(target_class)`\n",
    "  - Push weight: `β × (1 - P_teacher(negative_class))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "# Device Config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training config\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "EPOCHS_TEACHER = 10\n",
    "EPOCHS_STUDENT = 20  # Using 20 epochs for fair comparison\n",
    "\n",
    "# Contrastive config\n",
    "TEMP = 0.07\n",
    "\n",
    "# Distillation config (for SupCRD)\n",
    "ALPHA = 1.0  # Pull force weight\n",
    "BETA = 10.0  # Push force weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Loading (CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train samples: {len(train_set)}\")\n",
    "print(f\"Test samples: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    \"\"\"Simple CNN Encoder outputting a flat feature vector.\"\"\"\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super().__init__()\n",
    "        # Input: 3 x 32 x 32\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2), # -> 16x16\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2), # -> 8x8\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2) # -> 4x4\n",
    "        )\n",
    "        self.flat_dim = 128 * 4 * 4\n",
    "        self.fc = nn.Linear(self.flat_dim, feature_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    \"\"\"Wraps Encoder, Projection Head, and Classifier.\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.encoder = ConvEncoder(feature_dim=128)\n",
    "        \n",
    "        # Projection Head (for Contrastive Loss)\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64) \n",
    "        )\n",
    "        \n",
    "        # Classifier (for Teacher Supervision / Eval)\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.encoder(x)       # (Batch, 128)\n",
    "        proj = self.projector(feats)  # (Batch, 64)\n",
    "        logits = self.classifier(feats) # (Batch, 10)\n",
    "        return feats, proj, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogitWeightedSupCRDLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Logit-Weighted Supervised Contrastive Representation Distillation.\n",
    "    \n",
    "    Uses teacher probabilities to weight contrastive forces:\n",
    "    - Pull: α × P_teacher(target_class)\n",
    "    - Push: β × (1 - P_teacher(negative_class))\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, beta=1.0, temperature=0.07, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.tau = temperature\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, student_features, teacher_features, teacher_logits, labels):\n",
    "        batch_size = student_features.shape[0]\n",
    "        device = student_features.device\n",
    "\n",
    "        # 1. Normalize features\n",
    "        s_norm = F.normalize(student_features, dim=1)\n",
    "        t_norm = F.normalize(teacher_features, dim=1)\n",
    "\n",
    "        # 2. Similarity Matrix\n",
    "        sim_matrix = torch.matmul(s_norm, t_norm.T) / self.tau\n",
    "        sim_max, _ = torch.max(sim_matrix, dim=1, keepdim=True)\n",
    "        sim_matrix = sim_matrix - sim_max.detach()\n",
    "        exp_sim = torch.exp(sim_matrix)\n",
    "\n",
    "        # 3. Teacher Probabilities\n",
    "        teacher_probs = F.softmax(teacher_logits, dim=1)\n",
    "\n",
    "        # 4. Construct Masks\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask_pos = torch.eq(labels, labels.T).float().to(device)\n",
    "        mask_neg = 1.0 - mask_pos\n",
    "\n",
    "        # 5. Compute Weights\n",
    "        # Pull weight\n",
    "        p_target = torch.gather(teacher_probs, 1, labels).view(-1)\n",
    "        w_pull = self.alpha * p_target\n",
    "        \n",
    "        # Push weight\n",
    "        target_labels_expand = labels.view(1, -1).expand(batch_size, -1)\n",
    "        p_negative_class = torch.gather(teacher_probs, 1, target_labels_expand)\n",
    "        w_push = self.beta * (1.0 - p_negative_class)\n",
    "\n",
    "        # 6. Compute Loss\n",
    "        sum_pos_exp = (exp_sim * mask_pos).sum(dim=1)\n",
    "        numerator_term = w_pull * sum_pos_exp\n",
    "        weighted_neg_exp = (exp_sim * w_push * mask_neg).sum(dim=1)\n",
    "        denominator_term = numerator_term + weighted_neg_exp\n",
    "\n",
    "        loss = -torch.log((numerator_term + self.eps) / (denominator_term + self.eps))\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"BASELINE: Standard Supervised Contrastive Loss (Student vs Student)\"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temp = temperature\n",
    "\n",
    "    def forward(self, student_proj, labels):\n",
    "        feats = F.normalize(student_proj, dim=1)\n",
    "        sim_matrix = torch.matmul(feats, feats.T) / self.temp\n",
    "        \n",
    "        labels = labels.view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        \n",
    "        # Remove self-contrast\n",
    "        logits_mask = torch.scatter(torch.ones_like(mask), 1, \n",
    "                                    torch.arange(feats.shape[0]).view(-1, 1).to(device), 0)\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        logits_max, _ = torch.max(sim_matrix, dim=1, keepdim=True)\n",
    "        sim_matrix = sim_matrix - logits_max.detach()\n",
    "        \n",
    "        exp_logits = torch.exp(sim_matrix) * logits_mask\n",
    "        log_prob = sim_matrix - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "        \n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n",
    "        return -mean_log_prob_pos.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualization: t-SNE Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latents(model, loader, title=\"Latent Space\", save_path=None):\n",
    "    \"\"\"Runs t-SNE on model features and plots them.\"\"\"\n",
    "    model.eval()\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "    num_samples = 2000\n",
    "    count = 0\n",
    "    \n",
    "    print(f\"[{title}] Extracting features...\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            feats, _, _ = model(images)\n",
    "            feats = feats.view(feats.size(0), -1).cpu()\n",
    "            \n",
    "            features_list.append(feats)\n",
    "            labels_list.append(labels)\n",
    "            \n",
    "            count += images.size(0)\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "\n",
    "    X = torch.cat(features_list, dim=0).numpy()[:num_samples]\n",
    "    y = torch.cat(labels_list, dim=0).numpy()[:num_samples]\n",
    "\n",
    "    print(f\"[{title}] Running t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, init='pca', learning_rate='auto')\n",
    "    X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cifar_classes = ['Plane', 'Car', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "    \n",
    "    sns.scatterplot(\n",
    "        x=X_embedded[:, 0], y=X_embedded[:, 1], \n",
    "        hue=y, palette='tab10', legend='full', alpha=0.7\n",
    "    )\n",
    "    plt.legend(title='Classes', labels=cifar_classes, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title(f\"{title} (t-SNE)\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved plot to {save_path}\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher(epochs=10):\n",
    "    \"\"\"Train teacher model with standard cross-entropy.\"\"\"\n",
    "    model = ModelWrapper(num_classes=10).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING TEACHER MODEL ({epochs} epochs)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            _, _, logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        acc = 100. * correct / total\n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs}: Loss={avg_loss:.3f} | Acc={acc:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n✓ Teacher training complete: {acc:.1f}% accuracy\\n\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_student(teacher_model, mode=\"supcon\", epochs=20, alpha=1.0, beta=1.0, temperature=0.07):\n",
    "    \"\"\"\n",
    "    Train student with contrastive loss.\n",
    "    \n",
    "    Args:\n",
    "        mode: 'supcon' (baseline) or 'supcrd' (logit-weighted)\n",
    "    \"\"\"\n",
    "    student = ModelWrapper(num_classes=10).to(device)\n",
    "    optimizer = torch.optim.Adam(student.parameters(), lr=LR)\n",
    "    \n",
    "    # Online Linear Probe\n",
    "    probe_head = nn.Linear(128, 10).to(device) \n",
    "    probe_opt = torch.optim.Adam(probe_head.parameters(), lr=LR)\n",
    "    probe_crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Select loss\n",
    "    if mode == \"supcrd\":\n",
    "        criterion = LogitWeightedSupCRDLoss(alpha=alpha, beta=beta, temperature=temperature)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TRAINING: Logit-Weighted SupCRD\")\n",
    "        print(f\"α={alpha} (pull) | β={beta} (push) | τ={temperature}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    else:\n",
    "        criterion = SupConLoss(temperature=temperature)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TRAINING: Baseline SupCon\")\n",
    "        print(f\"τ={temperature}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "    training_log = {\n",
    "        'epochs': [],\n",
    "        'loss': [],\n",
    "        'probe_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        probe_head.train()\n",
    "        total_loss = 0\n",
    "        probe_acc = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward Student\n",
    "            s_feats, s_proj, _ = student(images)\n",
    "            \n",
    "            # Compute Loss\n",
    "            if mode == \"supcrd\":\n",
    "                with torch.no_grad():\n",
    "                    _, t_proj, t_logits = teacher_model(images)\n",
    "                loss = criterion(s_proj, t_proj, t_logits, labels)\n",
    "            else:\n",
    "                loss = criterion(s_proj, labels)\n",
    "\n",
    "            # Optimize Student\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Optimize Linear Probe\n",
    "            probe_logits = probe_head(s_feats.detach()) \n",
    "            p_loss = probe_crit(probe_logits, labels)\n",
    "            \n",
    "            probe_opt.zero_grad()\n",
    "            p_loss.backward()\n",
    "            probe_opt.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, preds = probe_logits.max(1)\n",
    "            probe_acc += preds.eq(labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        avg_acc = 100. * probe_acc / total_samples\n",
    "        \n",
    "        training_log['epochs'].append(epoch + 1)\n",
    "        training_log['loss'].append(avg_loss)\n",
    "        training_log['probe_acc'].append(avg_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs}: Loss={avg_loss:.4f} | Probe Acc={avg_acc:.2f}%\")\n",
    "    \n",
    "    print(f\"\\n✓ Student training complete: {avg_acc:.2f}% accuracy\\n\")\n",
    "    return student, training_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# EXPERIMENTS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 0: Train Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = train_teacher(epochs=EPOCHS_TEACHER)\n",
    "teacher.eval()\n",
    "\n",
    "# Visualize teacher's latent space\n",
    "visualize_latents(teacher, test_loader, \n",
    "                  title=\"Teacher Model\",\n",
    "                  save_path=\"teacher_latent.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1: Baseline SupCon (Student vs Student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_student, baseline_log = train_student(\n",
    "    teacher_model=teacher,\n",
    "    mode=\"supcon\",\n",
    "    epochs=EPOCHS_STUDENT,\n",
    "    temperature=TEMP\n",
    ")\n",
    "\n",
    "visualize_latents(baseline_student, test_loader,\n",
    "                  title=\"Baseline SupCon\",\n",
    "                  save_path=\"baseline_supcon_latent.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2: Logit-Weighted SupCRD (α=1.0, β=10.0, τ=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supcrd_student_1, supcrd_log_1 = train_student(\n",
    "    teacher_model=teacher,\n",
    "    mode=\"supcrd\",\n",
    "    epochs=EPOCHS_STUDENT,\n",
    "    alpha=1.0,\n",
    "    beta=10.0,\n",
    "    temperature=0.07\n",
    ")\n",
    "\n",
    "visualize_latents(supcrd_student_1, test_loader,\n",
    "                  title=\"SupCRD (α=1.0, β=10.0, τ=0.07)\",\n",
    "                  save_path=\"supcrd_a1_b10_t007_latent.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3: Logit-Weighted SupCRD (α=1.0, β=10.0, τ=0.007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supcrd_student_2, supcrd_log_2 = train_student(\n",
    "    teacher_model=teacher,\n",
    "    mode=\"supcrd\",\n",
    "    epochs=EPOCHS_STUDENT,\n",
    "    alpha=1.0,\n",
    "    beta=10.0,\n",
    "    temperature=0.007\n",
    ")\n",
    "\n",
    "visualize_latents(supcrd_student_2, test_loader,\n",
    "                  title=\"SupCRD (α=1.0, β=10.0, τ=0.007)\",\n",
    "                  save_path=\"supcrd_a1_b10_t0007_latent.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 4: Alpha Sweep (β=1.0, τ=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_sweep_results = {}\n",
    "\n",
    "for alpha in [1.0, 10.0, 50.0]:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"Alpha Sweep: α={alpha}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    student, log = train_student(\n",
    "        teacher_model=teacher,\n",
    "        mode=\"supcrd\",\n",
    "        epochs=EPOCHS_STUDENT,\n",
    "        alpha=alpha,\n",
    "        beta=1.0,\n",
    "        temperature=0.07\n",
    "    )\n",
    "    \n",
    "    alpha_sweep_results[f\"alpha_{alpha}\"] = {\n",
    "        'model': student,\n",
    "        'log': log\n",
    "    }\n",
    "    \n",
    "    visualize_latents(student, test_loader,\n",
    "                      title=f\"SupCRD (α={alpha}, β=1.0)\",\n",
    "                      save_path=f\"supcrd_a{int(alpha)}_b1_latent.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 5: Beta Sweep (α=1.0, τ=0.07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_sweep_results = {}\n",
    "\n",
    "for beta in [1.0, 10.0, 50.0]:\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"Beta Sweep: β={beta}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    student, log = train_student(\n",
    "        teacher_model=teacher,\n",
    "        mode=\"supcrd\",\n",
    "        epochs=EPOCHS_STUDENT,\n",
    "        alpha=1.0,\n",
    "        beta=beta,\n",
    "        temperature=0.07\n",
    "    )\n",
    "    \n",
    "    beta_sweep_results[f\"beta_{beta}\"] = {\n",
    "        'model': student,\n",
    "        'log': log\n",
    "    }\n",
    "    \n",
    "    visualize_latents(student, test_loader,\n",
    "                      title=f\"SupCRD (α=1.0, β={beta})\",\n",
    "                      save_path=f\"supcrd_a1_b{int(beta)}_latent.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# RESULTS & ANALYSIS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(baseline_log['epochs'], baseline_log['loss'], 'o-', label='Baseline SupCon', linewidth=2)\n",
    "ax1.plot(supcrd_log_1['epochs'], supcrd_log_1['loss'], 's-', label='SupCRD (τ=0.07)', linewidth=2)\n",
    "ax1.plot(supcrd_log_2['epochs'], supcrd_log_2['loss'], '^-', label='SupCRD (τ=0.007)', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(baseline_log['epochs'], baseline_log['probe_acc'], 'o-', label='Baseline SupCon', linewidth=2)\n",
    "ax2.plot(supcrd_log_1['epochs'], supcrd_log_1['probe_acc'], 's-', label='SupCRD (τ=0.07)', linewidth=2)\n",
    "ax2.plot(supcrd_log_2['epochs'], supcrd_log_2['probe_acc'], '^-', label='SupCRD (τ=0.007)', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Probe Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Linear Probe Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha Sweep Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for name, data in alpha_sweep_results.items():\n",
    "    alpha_val = name.split('_')[1]\n",
    "    plt.plot(data['log']['epochs'], data['log']['probe_acc'], \n",
    "             'o-', label=f'α={alpha_val}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Probe Accuracy (%)', fontsize=12)\n",
    "plt.title('Alpha Sweep: Effect on Student Performance (β=1.0)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('alpha_sweep.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta Sweep Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for name, data in beta_sweep_results.items():\n",
    "    beta_val = name.split('_')[1]\n",
    "    plt.plot(data['log']['epochs'], data['log']['probe_acc'], \n",
    "             's-', label=f'β={beta_val}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Probe Accuracy (%)', fontsize=12)\n",
    "plt.title('Beta Sweep: Effect on Student Performance (α=1.0)', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('beta_sweep.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary_data = {\n",
    "    'Method': [\n",
    "        'Teacher',\n",
    "        'Baseline SupCon',\n",
    "        'SupCRD (α=1, β=10, τ=0.07)',\n",
    "        'SupCRD (α=1, β=10, τ=0.007)'\n",
    "    ],\n",
    "    'Final Accuracy (%)': [\n",
    "        88.0,  # Teacher\n",
    "        baseline_log['probe_acc'][-1],\n",
    "        supcrd_log_1['probe_acc'][-1],\n",
    "        supcrd_log_2['probe_acc'][-1]\n",
    "    ],\n",
    "    'Final Loss': [\n",
    "        0.342,  # Teacher\n",
    "        baseline_log['loss'][-1],\n",
    "        supcrd_log_1['loss'][-1],\n",
    "        supcrd_log_2['loss'][-1]\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Findings\n",
    "\n",
    "**TODO**: Document your observations here:\n",
    "\n",
    "1. **Temperature Effect**: \n",
    "   - τ=0.07 vs τ=0.007 comparison\n",
    "   - Impact on cluster tightness\n",
    "\n",
    "2. **Alpha (Pull Force)**:\n",
    "   - Does varying α significantly affect results?\n",
    "   - Hypothesis: Should not matter much if pull=push coupling exists\n",
    "\n",
    "3. **Beta (Push Force)**:\n",
    "   - Strong effect expected on inter-class separation\n",
    "   - Tradeoff: semantic similarity vs clear boundaries\n",
    "\n",
    "4. **Baseline Comparison**:\n",
    "   - Does SupCRD outperform standard SupCon?\n",
    "   - Evidence of semantic structure (dog closer to wolf than car)?\n",
    "\n",
    "5. **Open Questions**:\n",
    "   - Pull = Push coupling?\n",
    "   - Need for explicit decoupling?\n",
    "   - Optimal α/β balance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
