# Logit-Weighted Supervised Contrastive Representation Distillation (LW-SupCRD)

## Project Overview
This project implements and evaluates **Logit-Weighted Supervised Contrastive Representation Distillation (LW-SupCRD)**, a novel knowledge distillation framework that combines supervised contrastive learning with teacher-guided semantic weighting on CIFAR-100.

**Key Innovation:** Uses teacher logits to semantically weight contrastive forces, achieving superior representation learning compared to standard supervised contrastive methods.

## Setup Instructions

### 1. Download Pre-trained Model Weights
**All pre-trained models are available on Google Drive:**

üîó **[Download Models Here](https://drive.google.com/drive/u/0/folders/1oyiYnKOiP7AYYiT7ik0Tq591gtPCJVAo)**

### 2. Create Directory Structure
Create a `pth_models/` folder at the project root:

```bash
mkdir pth_models
```

Your directory structure should look like:

```
ATML_Proj/
‚îÇ
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ DeSupCon.ipynb
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ proposal/
‚îÇ   ‚îú‚îÄ‚îÄ Decoupled Feature Distillation Idea Explanation.pdf
‚îÇ   ‚îî‚îÄ‚îÄ prop.tex (+ LaTeX auxiliary files)
‚îÇ
‚îú‚îÄ‚îÄ json_results/
‚îÇ   ‚îú‚îÄ‚îÄ comprehensive_results_resnet18_cifar100.json
‚îÇ   ‚îî‚îÄ‚îÄ training_logs/
‚îÇ       ‚îú‚îÄ‚îÄ teacher_resnet50_cifar100.json
‚îÇ       ‚îú‚îÄ‚îÄ student_baseline_supcon_resnet18_cifar100.json
‚îÇ       ‚îú‚îÄ‚îÄ student_baseline_crd_resnet18_cifar100.json
‚îÇ       ‚îú‚îÄ‚îÄ student_undistilled_resnet18_cifar100.json
‚îÇ       ‚îú‚îÄ‚îÄ student_alpha_*.json (Œ± sweep experiments)
‚îÇ       ‚îú‚îÄ‚îÄ student_*_beta_*.json (Œ≤ sweep experiments)
‚îÇ       ‚îú‚îÄ‚îÄ student_*_temp_*.json (temperature sweep)
‚îÇ       ‚îî‚îÄ‚îÄ student_hybrid_lambda_*.json (hybrid loss experiments)
‚îÇ
‚îú‚îÄ‚îÄ plots/
‚îÇ   ‚îú‚îÄ‚îÄ t-SNE visualizations (tsne_*.png)
‚îÇ   ‚îú‚îÄ‚îÄ 3D hypersphere distributions (*_hypersphere.html)
‚îÇ   ‚îî‚îÄ‚îÄ Alignment & Uniformity analyses (*_alignment.png)
‚îÇ
‚îî‚îÄ‚îÄ pth_models/
    ‚îú‚îÄ‚îÄ teacher_resnet50_cifar100.pth (80.75% acc)
    ‚îú‚îÄ‚îÄ teacher_resnet50_cifar100_with_projection.pth
    ‚îú‚îÄ‚îÄ student_baseline_supcon_resnet18_cifar100.pth (69.08%)
    ‚îú‚îÄ‚îÄ student_baseline_crd_resnet18_cifar100.pth (68.05%)
    ‚îú‚îÄ‚îÄ student_undistilled_resnet18_cifar100.pth (67.93%)
    ‚îú‚îÄ‚îÄ student_alpha_1.0_beta_10.0_temp_0.07_resnet18_cifar100.pth ‚≠ê (73.35% - BEST)
    ‚îî‚îÄ‚îÄ student_*_resnet18_cifar100.pth (various configurations)
```

### 3. Download Required Models

Download these essential models from Google Drive and place them in `pth_models/`:

#### Core Models (Required)
- `teacher_resnet50_cifar100.pth` - Teacher model (80.75% accuracy)
- `teacher_resnet50_cifar100_with_projection.pth` - Teacher with trained 64-dim cosine projection
- `student_baseline_supcon_resnet18_cifar100.pth` - Baseline SupCon student (69.08%)

#### Best Model (Recommended) üèÜ
- `student_alpha_1.0_beta_10.0_temp_0.07_resnet18_cifar100.pth` - **73.35% accuracy** (best overall)

#### Baselines & Ablations (Optional)
- `student_baseline_crd_resnet18_cifar100.pth` - Baseline CRD (68.05%)
- `student_undistilled_resnet18_cifar100.pth` - Undistilled student (67.93%)
- Various Œ±, Œ≤, temperature, and hybrid configurations

---

## Results Summary

### **Main Results - Best Configurations**

| Method | Test Acc | Œî vs SupCon | Alignment ‚Üì | Uniformity ‚Üì | Key Features |
|--------|----------|-------------|-------------|--------------|--------------|
| **üèÜ LW-SupCRD (œÑ=0.07)** | **73.35%** | **+4.27%** | 1.1990 | **-3.7104** | Best overall - optimal temperature |
| LW-SupCRD (Œ±=1, Œ≤=10) | 73.19% | +4.11% | 1.1518 | **-3.7027** | Near-identical to œÑ=0.07 |
| Hybrid (Œª=0.3) | 72.58% | +3.50% | **0.6043** | -3.2880 | Best hybrid - severe overfitting |
| **Baseline SupCon** | 69.08% | - | **0.4377** | -2.5665 | Strong alignment, weak uniformity |
| Baseline CRD | 68.05% | -1.03% | 0.9008 | -2.2358 | Poor both metrics |
| Undistilled Student | 67.93% | -1.15% | 0.6631 | -1.7332 | Terrible uniformity |
| **Teacher (ResNet-50)** | 80.75% | +11.67% | **0.5928** | **-3.4649** | Reference upper bound |

**Key Takeaway:** LW-SupCRD achieves **73.35%** with best-in-class uniformity (**-3.7104**), even surpassing the teacher's uniformity (-3.4649), while maintaining competitive alignment for superior generalization.

---

### **Comprehensive Experimental Results**

#### 1. **Alpha (Œ±) Sweep - Pull Force Weighting**
*Configuration: Œ≤=10, œÑ=0.07, adaptive Œ≤*

| Œ± | Test Acc | Œî vs SupCon | Alignment ‚Üì | Uniformity ‚Üì | Observation |
|---|----------|-------------|-------------|--------------|-------------|
| **1.0** | **73.19%** | **+4.11%** | 1.1518 | **-3.7027** | ‚úÖ Optimal balance |
| 2.0 | 71.78% | +2.70% | **1.1129** | -3.6744 | Tighter clusters ‚Üí worse uniformity |
| 5.0 | 70.67% | +1.59% | 1.1589 | -3.6712 | Over-clustering |
| 10.0 | 70.07% | +0.99% | 1.2754 | -3.6728 | Severe over-clustering |

**Finding:** Œ±=1 optimal - higher Œ± causes tighter clusters, sacrificing uniformity and causing overfitting.

---

#### 2. **Beta (Œ≤) Sweep - Push Force Strength**
*Configuration: Œ±=1, œÑ=0.07, adaptive Œ≤*

| Œ≤ | Test Acc | Œî vs SupCon | Alignment ‚Üì | Uniformity ‚Üì | Observation |
|---|----------|-------------|-------------|--------------|-------------|
| **10.0** | **73.19%** | **+4.11%** | **1.1518** | **-3.7027** | ‚úÖ Optimal - strong push |
| 12.0 | 71.31% | +2.23% | 1.2068 | -3.6654 | Too strong ‚Üí degradation |
| 5.0 | 70.63% | +1.55% | 1.2487 | -3.6785 | Weak push ‚Üí poor separation |
| 1.0 | 70.46% | +1.38% | 1.1862 | -3.6390 | Very weak push |

**Finding:** Œ≤=10 optimal - balances strong class separation with stable training. Too high causes instability, too low fails to separate classes.

---

#### 3. **Temperature (œÑ) Sweep - Gradient Sharpness**
*Configuration: Œ±=1, Œ≤=10, adaptive Œ≤*

| œÑ | Test Acc | Œî vs SupCon | Alignment ‚Üì | Uniformity ‚Üì | Observation |
|---|----------|-------------|-------------|--------------|-------------|
| **0.07** | **73.35%** | **+4.27%** | 1.1990 | **-3.7104** | ‚úÖ Optimal - balanced spread |
| 0.05 | 68.08% | -1.00% | 1.3503 | -3.6645 | Too sharp ‚Üí poor alignment |

**Finding:** œÑ=0.07 provides optimal gradient flow - œÑ=0.05 too sharp, only closest pairs contribute.

---

#### 4. **Hybrid Loss (Œª) Sweep - SupCon + LW-SupCRD Mix**
*Formula: `L = Œª √ó SupCon + (1-Œª) √ó LW-SupCRD`*
*Configuration: Œ±=1, Œ≤=10, œÑ=0.07*

| Œª | Test Acc | Train Acc | Gap | Alignment ‚Üì | Uniformity ‚Üì | Observation |
|---|----------|-----------|-----|-------------|--------------|-------------|
| **0.3** | **72.58%** | 98.73% | **26.15%** | **0.6043** | -3.2880 | Best hybrid - severe overfitting |
| 0.5 | 72.07% | 98.92% | 26.85% | **0.5166** | -2.9451 | Worse overfitting |
| 0.7 | 71.57% | 98.25% | 26.68% | **0.4845** | -2.8290 | Poor uniformity |
| 0.9 | 70.69% | 95.40% | 24.71% | **0.4394** | -2.6273 | Approaching pure SupCon |

**Critical Finding:** All hybrids show massive overfitting (24-27% gap) despite excellent alignment. **Pure LW-SupCRD (73.35%) beats best hybrid (72.58%)** - adding SupCon only adds noise.

---

## Key Findings

### **1. The Alignment-Uniformity Trade-off for CIFAR-100** üìä

For fine-grained classification (100 classes), **uniformity is more critical than tight alignment:**

**Best Methods (73%+):**
- Alignment: ~1.15-1.20 (moderate clusters)
- Uniformity: ~-3.70 (excellent spread)
- Strategy: Trade cluster tightness for class separation

**Worst Methods (68%-):**
- Alignment: ~0.44-0.66 (very tight clusters)
- Uniformity: ~-2.50 to -1.73 (poor spread)
- Problem: Over-clustering causes poor separation

**Counter-intuitive Insight:** Student's "worse" alignment (1.20 vs teacher's 0.59) actually helps generalization by maintaining better class separation on the hypersphere.

---

### **2. Hyperparameter Roles & Interactions**

**Œ± (Pull Weight) - Semantic Confidence:**
- Controls cluster tightness via teacher probabilities
- Œ±=1 optimal: Minimal semantic weighting
- Higher Œ± ‚Üí tighter clusters ‚Üí worse uniformity ‚Üí overfitting
- Effect: Primarily degrades uniformity

**Œ≤ (Push Weight) - Negative Force Strength:**
- Controls class separation strength
- Œ≤=10 optimal: Strong push forces
- Critical discovery: Affects **both** alignment AND uniformity simultaneously
- Unlike Œ±, strong Œ≤ improves both metrics

**œÑ (Temperature) - Gradient Sharpness:**
- Controls exponential scaling in similarity
- œÑ=0.07 optimal: Balanced gradient flow
- œÑ=0.05 too sharp: Only nearest neighbors contribute
- Effect: Primarily affects uniformity

**Adaptive Œ≤ - Curriculum Learning:**
- Early epochs (uncertain): Œ≤_eff = 1.25Œ≤ (stronger push)
- Late epochs (confident): Œ≤_eff = 0.71Œ≤ (weaker push)
- Provides natural hard negative mining

---

### **3. Why Hybrids Fail** ‚ùå

All hybrid losses (Œª=0.3 to 0.9) show:
- ‚úì Excellent alignment (0.44-0.60, like teacher)
- ‚úó Poor uniformity (-2.6 to -3.3)
- ‚úó Massive overfitting (24-27% train-test gap)
- ‚úó Lower accuracy than pure LW-SupCRD

**Root Cause:** SupCon's pull-only forces create over-tight clusters, sacrificing the uniformity that LW-SupCRD's strong push forces (Œ≤=10) achieve.

**Conclusion:** Pure LW-SupCRD (73.35%) > Best Hybrid (72.58%)

---

### **4. Student Surpasses Teacher in Uniformity** üéØ

| Metric | Teacher | Best Student | Observation |
|--------|---------|--------------|-------------|
| Alignment | **0.5928** | 1.1990 | Student 2√ó looser |
| Uniformity | -3.4649 | **-3.7104** | Student 7% better |
| Accuracy | 80.75% | 73.35% | Reasonable gap |

**Key Insight:** Student trades alignment for uniformity and still outperforms all baselines significantly. The looser clusters + better spread = superior linear separability.

---

### **5. Gradient Normalization Critical** ‚öôÔ∏è

The `/Œ±` normalization in the loss prevents gradient saturation:
- Without: Œ±=2 causes exponentials ~exp(20) = 4.8√ó10‚Å∏
- With: Allows proper Œ± scaling without optimization collapse
- Enables exploration of Œ±>1 configurations

This fix was essential for all Œ± sweep experiments to work.

---

## Technical Details

### Model Architecture
- **Teacher:** ResNet-50 (23.5M parameters, 80.75% accuracy)
- **Student:** ResNet-18 (11.2M parameters)
- **Projection:** 2048-dim backbone ‚Üí 64-dim contrastive space
- **Dataset:** CIFAR-100 (100 classes, 50k train / 10k test)
- **Training:** 50 epochs, batch size 128, Adam optimizer (lr=1e-3)

### Loss Functions

#### 1. **Baseline SupCon** (Khosla et al., 2020)
Standard supervised contrastive learning - pull positives only.

#### 2. **Baseline CRD** (Tian et al., 2020)
Contrastive Representation Distillation - instance matching.

#### 3. **LW-SupCRD** (Ours)
Logit-weighted supervised contrastive with adaptive forces:

```python
# Pull weight (semantic confidence)
w_pull = Œ± √ó p_teacher(correct_class)

# Push weight (inverse adaptive)
if adaptive_beta:
    Œ≤_effective = Œ≤ / (p_target + 0.5)
    w_push = Œ≤_effective √ó (1 - p_teacher(negative_class))
else:
    w_push = Œ≤ √ó (1 - p_teacher(negative_class))

# Gradient normalization
loss = -log((w_pull √ó pos_exp) / (w_pull √ó pos_exp + w_push √ó neg_exp))
loss = loss / Œ±  # CRITICAL: prevents gradient saturation
```

#### 4. **Hybrid Loss**
```python
L = Œª √ó L_SupCon + (1 - Œª) √ó L_LW-SupCRD
```
Best: Œª=0.3, but still underperforms pure LW-SupCRD.

---

## Visualization & Analysis

### Alignment & Uniformity Metrics (Wang & Isola, 2020)

**Alignment Loss (‚Üì better):**
```
L_align = E[||f(x) - f(x+)||¬≤]
```
Measures positive pair distance - lower = tighter clusters.

**Uniformity Loss (‚Üì better, more negative):**
```
L_uniform = log(E[exp(-2||f(x) - f(y)||¬≤)])
```
Measures hypersphere coverage - more negative = better spread.

### Available Visualizations

All experiments include:
- **t-SNE plots:** 2D projection of learned representations (20 classes)
  - Static PNG files: `plots/tsne_*.png`
  - Open directly in any image viewer
  
- **3D Hypersphere:** Interactive Plotly visualizations (`.html` files)
  - Interactive HTML files: `plots/*_hypersphere.html`
  - **How to view:** Simply double-click the `.html` file to open in your web browser
  - Features: Rotate with mouse, zoom with scroll, click legend to toggle classes
  - Shows both "All Classes" and "10 Random Classes" views side-by-side
  - Example files:
    - `plots/alpha_1.0_hypersphere.html` - Best Œ± configuration
    - `plots/temp_0.07_hypersphere.html` - Best temperature (73.35% model)
    - `plots/baseline_supcon_hypersphere.html` - Baseline comparison
  
- **Alignment/Uniformity:** Comprehensive Wang & Isola analysis
  - Static PNG files: `plots/*_alignment.png`
  - Shows histograms, density plots, and per-class angular distributions
  
- **Training logs:** JSON files with per-epoch metrics
  - Location: `json_results/training_logs/*.json`
  - Contains: epoch-by-epoch loss, accuracy, learning rate

---

## Dependencies

```bash
pip install torch torchvision
pip install numpy matplotlib scikit-learn scipy
pip install plotly  # For interactive 3D visualizations
pip install tqdm
```

---

## Viewing Interactive Visualizations

### 3D Hypersphere Plots (HTML Files)

The repository includes **interactive 3D hypersphere visualizations** that show how representations are distributed on the unit sphere:

**To view:**
1. Navigate to the `plots/` folder
2. Double-click any `*_hypersphere.html` file
3. Your default web browser will open the visualization

**Interactive controls:**
- **Rotate:** Click and drag with mouse
- **Zoom:** Scroll wheel
- **Pan:** Right-click and drag (or Shift + drag)
- **Toggle classes:** Click legend items to show/hide classes
- **Reset view:** Double-click the plot

**Recommended visualizations to explore:**
```
plots/temp_0.07_hypersphere.html          # üèÜ Best model (73.35%)
plots/baseline_supcon_hypersphere.html     # Compare with baseline (69.08%)
plots/undistilled_hypersphere.html         # See the improvement
plots/hybrid_lambda_0.3_hypersphere.html   # Best hybrid (overfitting example)
```

**What to look for:**
- **Left panel:** All 100 classes - should show good spread across sphere
- **Right panel:** 10 random classes - shows cluster separation clearly
- **Color gradient:** Represents class labels (0-99)
- **Point density:** Tighter clusters = better alignment, spread = better uniformity

**Browser compatibility:** Works best in Chrome/Firefox/Edge. Safari may have minor rendering differences.

---

## Usage

### Running the Notebook

1. Open `DeSupCon.ipynb` in Jupyter
2. Download required models from Google Drive
3. Place models in `pth_models/` directory
4. Run all cells sequentially

**Training Control:**
- Set `FORCE_RETRAIN = True` to retrain models (ignores cached weights)
- Set `FORCE_RETRAIN = False` to load pre-trained models (default)

### Loading Best Model

```python
import torch
from models import ModelWrapper

# Load best model
model = ModelWrapper(num_classes=100, arch='resnet18')
checkpoint = torch.load('pth_models/student_alpha_1.0_beta_10.0_temp_0.07_resnet18_cifar100.pth')
model.load_state_dict(checkpoint)
model.eval()

# Inference
with torch.no_grad():
    features, projections, logits = model(images)
```

---

## Experimental Protocol

### Teacher Training
1. Train ResNet-50 on CIFAR-100 ‚Üí 80.75% accuracy
2. Train cosine similarity projection head (2048‚Üí64D)
3. Joint training: projection adapts during student training (CRD-style)

### Student Training
1. Multi-view augmentation (2 views per sample)
2. Contrastive loss on encoder projections
3. Separate linear classifier on frozen features (standard evaluation)
4. 50 epochs, batch size 128, Adam (lr=1e-3)

### Comprehensive Analysis Per Experiment
- t-SNE visualizations (20 sample classes)
- 3D hypersphere distribution (interactive HTML)
- Wang & Isola alignment-uniformity metrics
- Intra/inter-class distance analysis
- Separation ratio computation
- Training curves (JSON logs)
- Model checkpointing for reproducibility

---

## Citation

If you use this code or findings in your research, please cite:

```bibtex
@misc{lw_supcrd2024,
  title={Logit-Weighted Supervised Contrastive Representation Distillation: 
         Achieving Superior Uniformity through Semantic Force Weighting},
  author={Ibrahim Murtaza, Jibran Mazhar, Muhammad Ahsan Salar Khan},
  year={2024},
  institution={Lahore University of Management Sciences (LUMS)},
  course={EE-5102/CS-6304: Advanced Topics in Machine Learning},
  instructor={Professor Muhammad Tahir},
  note={Best Configuration: Œ±=1.0, Œ≤=10.0, œÑ=0.07 achieving 73.35% on CIFAR-100}
}
```

### Key References
- Khosla et al., "Supervised Contrastive Learning", NeurIPS 2020
- Wang & Isola, "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere", ICML 2020
- Tian et al., "Contrastive Representation Distillation", ICLR 2020

---

## Reproducibility

### Hardware
- **GPU:** RTX Pro 6000 Blackwell Edition
- **VRAM:** 96GB
- **Compute:** 119 TFLOPs
- **Training Time:** ~2-3 hours per configuration

### Random Seeds
All experiments use fixed random seeds for reproducibility:
```python
torch.manual_seed(42)
np.random.seed(42)
```

### Model Weights Distribution
All trained models available on Google Drive with:
- Model checkpoints (`.pth` files)
- Training logs (`.json` files)
- Comprehensive visualizations (`.png`, `.html`)

---

## Future Work

1. **Extended Architectures:** Test on deeper networks (ResNet-101, WideResNet)
2. **Larger Datasets:** Evaluate on ImageNet, iNaturalist
3. **Multi-Teacher:** Ensemble knowledge from multiple teachers
4. **Theoretical Analysis:** Formal proof of alignment-uniformity trade-off
5. **Publication:** Prepare for submission to WACV/BMVC

---

## Acknowledgments

- **Course Instructor:** Professor Muhammad Tahir
- **Team Members:** Ibrahim Murtaza, Jibran Mazhar, Muhammad Ahsan Salar Khan
- **Institution:** Lahore University of Management Sciences (LUMS)
- **Hardware Support:** RTX Pro 6000 Blackwell Edition (96GB VRAM)

Special thanks to:
- Khosla et al. for Supervised Contrastive Learning
- Wang & Isola for the alignment-uniformity framework
- Tian et al. for Contrastive Representation Distillation

---

## License
This project is for academic purposes as part of the ATML course at LUMS.

## Contact
For questions or issues, please open an issue on the repository or contact the team members.

---

**Last Updated:** December 23, 2024

**Status:** ‚úÖ All experiments completed | üìä Results finalized | üéØ Best model: 73.35% accuracy